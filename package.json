{
  "name": "hanzi-tokenizer",
  "version": "1.0.0",
  "description": "Convert Chinese text to list of Chinese words",
  "keywords": [
    "chinese",
    "tokens",
    "hanzi",
    "mandarin",
    "pinyin"
  ],
  "homepage": "https://github.com/pepebecker/hanzi-tokenizer#readme",
  "bugs": {
    "url": "https://github.com/pepebecker/hanzi-tokenizer/issues"
  },
  "license": "MIT",
  "author": "Pepe Becker <mail@pepebecker.com> (http://pepebecker.com)",
  "files": [],
  "main": "index.js",
  "repository": {
    "type": "git",
    "url": "https://github.com/pepebecker/hanzi-tokenizer.git"
  },
  "scripts": {
    "test": "mocha --timeout 20000"
  },
  "dependencies": {
    "mdbg": "^1.10.0"
  },
  "devDependencies": {
    "mocha": "^5.1.1",
    "should": "^13.2.1"
  }
}
